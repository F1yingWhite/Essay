> [元学习 — PaddleEdu documentation (paddlepedia.readthedocs.io)](https://paddlepedia.readthedocs.io/en/latest/tutorials/meta_learning/preliminaries.html#id2):迁移元学习，meta-learning，（learning to learn），直译即为学会学习，是一种“能力习得”途径。

元学习元学习 (Meta-Learning) 通常被理解为“学会学习 (Learning-to-Learn)”， 指的是在多个学习阶段改进学习算法的过程。 在基础学习过程中， 内部（或下层/基础）学习算法解决由数据集和目标定义的任务。 在元学习过程中，外部（或上层/元）算法更新内部学习算法，使其学习的模型改进外部目标。 因此，元学习的核心想法是学习一个先验知识 (prior)。
## 元数据含义
元学习的含义有两层，第一层是让机器学会学习，使其具备分析和解决问题的能力，机器通过完成任务获取经验，提高完成任务的能力; 第二层是让机器学习模型可以更好地泛化到新领域中，从而完成差异很大的新任务。
Few-Shot Learning 是 Meta-Learning 在监督学习领域的应用。 在 Meta-training 阶段， 将数据集分解为不同的任务，去学习类别变化的情况下模型的泛化能力。 在 Meta-testing 阶段， 面对全新的类别，不需要变动已有的模型，只需要通过一部或者少数几步训练，就可以完成需求。
总的来说就是先训练再泛化
## 元学习的单位
在元学习中，之前学习的任务称为元训练任务 (meta-train task)， 遇到的新任务称为元测试任务 (meta-test task)。 每个任务都有自己的训练集和测试集， 内部的训练集和测试集一般称为支持集 (Support Set) 和查询集 (Query Set)。 支持集又是一个 N-Way K-Shot 问题，即有 N 个类别，每个类有 K 个样例。
![[Pasted image 20240520204226.png]]
## 基学习器和元学习群
元学习本质上是层次优化问题 (双层优化问题 Bilevel Optimization Problem)， 其中一个优化问题嵌套在另一个优化问题中。 外部优化问题和内部优化问题通常分别称为上层优化问题和下层优化问题， 如图2所示的MAML。![[Pasted image 20240520204608.png]]
两层优化问题涉及两个参与器：
1. 上层的参与者是元学习器，
2. 下层的参与者是基学习器。 元学习器的最优决策依赖于基学习器的反应，基学习器自身会优化自己内部的决策。 这两个层次有各自不同的目标函数、约束条件和决策变量。 基学习器和元学习器的作用对象及功能如图3所示。
元学习器总结任务经验进行任务之间的共性学习，同时指导基学习器对新任务进行特性学习。
### 基学习器
- 基学习器就是考虑单个任务上的数据集,回答任务需要解决的问题
- 在单个任务上训练完成后，将训练的模型和参数都反馈给元学习器
### 元学习器
对所有任务进行归纳总结,综合新的经验
- 综合多个任务上基学习器训练的结果。
- 对多个任务的共性进行归纳，在新任务上进行快速准确的推理， 并且将推理输送给基学习器，作为初始模型和初始参数值， 或者是其他可以加速基学习器训练的参数。
- 指引基学习器的最优行为或探索某个特定的新任务。
- 提取任务上与模型和训练相关的特征。

## 元学习器工作原理
得到学习器F,让基学习器f在F的指导下通过几步微调就可以得到适应当前新任务的最优状态$f^*$.而𝐹的优化需要当前所有任务损失的累计和，

元学习的关键在于发现不同问题之间的普适规律，通过推广普适规律解决末知难题。普适规律需要达到对问题共性和特性表示力的均衡.


Meta Learning希望使得模型获取一种“学会学习”的能力，使其可以在获取已有“知识”的基础上快速学习新的任务，如：
- 让Alphago迅速学会下象棋
- 让一个猫咪图片分类器，迅速具有分类其他物体的能力
如果我们要进行N-ways，K-shot（数据中包含N个字符类别，每个字符有K张图像）的一个图像分类任务。比如20-ways，1-shot分类的意思是说，要做一个20分类，但是每个分类下只有1张图像的任务。我们可以依据Omniglot构建很多N-ways，K-shot任务，这些任务将作为元学习的任务来源。构建的任务分为训练任务（Train Task），测试任务（Test Task）。特别地，每个任务包含自己的**训练数据、测试数据**，在元学习里，分别称为**Support Set和Query Set**。
就是先把一个大的数据集拆分为test和train,每个train都有自己的test和train.**MAML的目的是获取一组更好的模型初始化参数（即让模型自己学会初始化）**。我们通过（许多）N-ways，K-shot的任务（训练任务）进行元学习的训练，使得模型学习到“先验知识”（初始化的参数）。这个“先验知识”在新的N-ways，K-shot任务上可以表现的更好。
![[Pasted image 20240525210806.png]]