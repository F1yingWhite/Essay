`WSI`:数字切片**是**利用全自动显微镜扫描系统，结合虚拟切片软件系统，把传统玻璃切片进行扫描、无缝拼接，生成一整张全视野（Whole Slide Image, 简称**WSI**）的数字切片（也称虚拟切片）。
病理科是医院医疗水平的重要代表，但非医学专业人士，甚至有些医学专业人士对它并不熟悉，经常将其混淆为“病历科”、“检验科”。那么，什么是病理呢？病理是研究疾病为何发生、如何发展和结局，机体有何变化，从而揭示疾病规律、阐明疾病本质的医学科学。病理科则利用医疗上获取的组织或细胞来进行疾病诊断。医学上把病理诊断作为最权威的诊断，并将其形象地称为“诊断的金标准”。  
诊断是治疗的基础，没有正确的诊断就没有有效的治疗，因此病理检查对于患者来说是必须的。一直以来，病理诊断在肿瘤诊断中有着不可替代的作用；而随着医学的进步，在更多的非肿瘤疾病的诊断中也越来越依赖病理，包括此次新冠肺炎疫情中，病理学也发挥了重要的作用。  
随着技术发展，病理医生的责任已不单纯是病理诊断，各种生物标志物的检测、筛选适合各种治疗的患者、判断患者的预后都成为病理医生的职责。
## 新的算法出现啦!
[binli123/dsmil-wsi: DSMIL: Dual-stream multiple instance learning networks for tumor detection in Whole Slide Image (github.com)](https://github.com/binli123/dsmil-wsi)
使用MIL算法进行全像素的运行，可以识别出其中的阳性与阴性区域
在诊断中，病理中阳性的区域范围其实少于20%，因此需要专家来进行标注，这种标注费时费力。
把一个wsi看成一个含有多个patch的bag，如果其中有一个阳性则阳性。patch级别的特征被提取然后聚类，最终变成slide级别的标签
但是当前有两个主要问题
- 如果阳性的切片非常少，使用简单的聚类操作的时候比如最大池化，会把模型分类分错。这是因为，在MIL的假设下，与完全监督的训练相比，最大池化会导致决策边界的转移
- 

MIL公式

## 病理预处理
### 病理切片
[【论文笔记】通过可解释的深度学习模型从病理切片识别癌症分子亚型_病理切片 深度学习 论文-CSDN博客](https://blog.csdn.net/qq_45893491/article/details/135285416)
根据这篇论文来看,
- 利用常规的阈值法自动分割WSI的组织部分，将每个WSI的组织区域切割为224×224的patch（每个WSI约有26000个patch）
什么是常规的阈值法自动分割?
通用代码如下:
preprocess.py
```python
import time

import cv2
import h5py
import numpy as np
import openslide
import torch
from PIL import ImageDraw
from shapely.affinity import scale
from shapely.geometry import Polygon
from shapely.ops import unary_union
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms


def segment_tissue(img):
    img_hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
    mthresh = 7
    img_med = cv2.medianBlur(img_hsv[:, :, 1], mthresh)
    _, img_prepped = cv2.threshold(img_med, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY)

    close = 4
    kernel = np.ones((close, close), np.uint8)
    img_prepped = cv2.morphologyEx(img_prepped, cv2.MORPH_CLOSE, kernel)

    # Find and filter contours
    contours, hierarchy = cv2.findContours(
        img_prepped, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE
    )
    return contours, hierarchy


def detect_foreground(contours, hierarchy):
    hierarchy = np.squeeze(hierarchy, axis=(0,))[:, 2:]

    # find foreground contours (parent == -1)
    hierarchy_1 = np.flatnonzero(hierarchy[:, 1] == -1)
    foreground_contours = [contours[cont_idx] for cont_idx in hierarchy_1]

    all_holes = []
    for cont_idx in hierarchy_1:
        all_holes.append(np.flatnonzero(hierarchy[:, 1] == cont_idx))

    hole_contours = []
    for hole_ids in all_holes:
        holes = [contours[idx] for idx in hole_ids]
        hole_contours.append(holes)

    return foreground_contours, hole_contours


def construct_polygon(foreground_contours, hole_contours, min_area):
    polys = []
    for foreground, holes in zip(foreground_contours, hole_contours):
        # We remove all contours that consist of fewer than 3 points, as these won't work with the Polygon constructor.
        if len(foreground) < 3:
            continue

        # remove redundant dimensions from the contour and convert to Shapely Polygon
        poly = Polygon(np.squeeze(foreground))

        # discard all polygons that are considered too small
        if poly.area < min_area:
            continue

        if not poly.is_valid:
            # This is likely becausee the polygon is self-touching or self-crossing.
            # Try and 'correct' the polygon using the zero-length buffer() trick.
            # See https://shapely.readthedocs.io/en/stable/manual.html#object.buffer
            poly = poly.buffer(0)

        # Punch the holes in the polygon
        for hole_contour in holes:
            if len(hole_contour) < 3:
                continue

            hole = Polygon(np.squeeze(hole_contour))

            if not hole.is_valid:
                continue

            # ignore all very small holes
            if hole.area < min_area:
                continue

            poly = poly.difference(hole)

        polys.append(poly)

    if len(polys) == 0:
        raise Exception("Raw tissue mask consists of 0 polygons")

    # If we have multiple polygons, we merge any overlap between them using unary_union().
    # This will result in a Polygon or MultiPolygon with most tissue masks.
    return unary_union(polys)


def generate_tiles(
    tile_width_pix, tile_height_pix, img_width, img_height, offsets=[(0, 0)]
):
    # Generate tiles covering the entire image.
    # Provide an offset (x,y) to create a stride-like overlap effect.
    # Add an additional tile size to the range stop to prevent tiles being cut off at the edges.
    range_stop_width = int(np.ceil(img_width + tile_width_pix))
    range_stop_height = int(np.ceil(img_height + tile_height_pix))

    rects = []
    for xmin, ymin in offsets:
        cols = range(int(np.floor(xmin)), range_stop_width, tile_width_pix)
        rows = range(int(np.floor(ymin)), range_stop_height, tile_height_pix)
        for x in cols:
            for y in rows:
                rect = Polygon(
                    [
                        (x, y),
                        (x + tile_width_pix, y),
                        (x + tile_width_pix, y - tile_height_pix),
                        (x, y - tile_height_pix),
                    ]
                )
                rects.append(rect)
    return rects


def make_tile_QC_fig(tiles, slide, level, line_width_pix=1, extra_tiles=None):
    # Render the tiles on an image derived from the specified zoom level
    img = slide.read_region((0, 0), level, slide.level_dimensions[level])
    downsample = 1 / slide.level_downsamples[level]

    draw = ImageDraw.Draw(img, "RGBA")
    for tile in tiles:
        bbox = tuple(np.array(tile.bounds) * downsample)
        draw.rectangle(bbox, outline="lightgreen", width=line_width_pix)

    # allow to display other tiles, such as excluded or sampled
    if extra_tiles:
        for tile in extra_tiles:
            bbox = tuple(np.array(tile.bounds) * downsample)
            draw.rectangle(bbox, outline="blue", width=line_width_pix + 1)

    return img


def create_tissue_mask(wsi, seg_level):
    # Determine the best level to determine the segmentation on
    level_dims = wsi.level_dimensions[seg_level]

    img = np.array(wsi.read_region((0, 0), seg_level, level_dims))

    # Get the total surface area of the slide level that was used
    level_area = level_dims[0] * level_dims[1]

    # Minimum surface area of tissue polygons (in pixels)
    # Note that this value should be sensible in the context of the chosen tile size
    min_area = level_area / 500

    contours, hierarchy = segment_tissue(img)
    foreground_contours, hole_contours = detect_foreground(contours, hierarchy)
    tissue_mask = construct_polygon(foreground_contours, hole_contours, min_area)

    # Scale the tissue mask polygon to be in the coordinate space of the slide's level 0
    scale_factor = wsi.level_downsamples[seg_level]
    tissue_mask_scaled = scale(
        tissue_mask, xfact=scale_factor, yfact=scale_factor, zfact=1.0, origin=(0, 0)
    )

    return tissue_mask_scaled


def create_tissue_tiles(
    wsi, tissue_mask_scaled, tile_size_microns, offsets_micron=None
):

    print(f"tile size is {tile_size_microns} um")

    # Compute the tile size in pixels from the desired tile size in microns and the image resolution
    assert (
        openslide.PROPERTY_NAME_MPP_X in wsi.properties
    ), "microns per pixel along X-dimension not available"
    assert (
        openslide.PROPERTY_NAME_MPP_Y in wsi.properties
    ), "microns per pixel along Y-dimension not available"

    mpp_x = float(wsi.properties[openslide.PROPERTY_NAME_MPP_X])
    mpp_y = float(wsi.properties[openslide.PROPERTY_NAME_MPP_Y])
    mpp_scale_factor = min(mpp_x, mpp_y)
    if mpp_x != mpp_y:
        print(
            f"mpp_x of {mpp_x} and mpp_y of {mpp_y} are not the same. Using smallest value: {mpp_scale_factor}"
        )

    tile_size_pix = round(tile_size_microns / mpp_scale_factor)

    # Use the tissue mask bounds as base offsets (+ a margin of a few tiles) to avoid wasting CPU power creating tiles that are never going
    # to be inside the tissue mask.
    tissue_margin_pix = tile_size_pix * 2
    minx, miny, maxx, maxy = tissue_mask_scaled.bounds
    min_offset_x = minx - tissue_margin_pix
    min_offset_y = miny - tissue_margin_pix
    offsets = [(min_offset_x, min_offset_y)]

    if offsets_micron is not None:
        assert (
            len(offsets_micron) > 0
        ), "offsets_micron needs to contain at least one value"
        # Compute the offsets in micron scale
        offset_pix = [round(o / mpp_scale_factor) for o in offsets_micron]
        offsets = [(o + min_offset_x, o + min_offset_y) for o in offset_pix]

    # Generate tiles covering the entire WSI
    all_tiles = generate_tiles(
        tile_size_pix,
        tile_size_pix,
        maxx + tissue_margin_pix,
        maxy + tissue_margin_pix,
        offsets=offsets,
    )

    # Retain only the tiles that sit within the tissue mask polygon
    filtered_tiles = [rect for rect in all_tiles if tissue_mask_scaled.intersects(rect)]

    return filtered_tiles


def tile_is_not_empty(tile, threshold_white=20):
    histogram = tile.histogram()

    # Take the median of each RGB channel. Alpha channel is not of interest.
    # If roughly each chanel median is below a threshold, i.e close to 0 till color value around 250 (white reference) then tile mostly white.
    whiteness_check = [0, 0, 0]
    for channel_id in (0, 1, 2):
        whiteness_check[channel_id] = np.median(
            histogram[256 * channel_id : 256 * (channel_id + 1)][100:200]
        )

    if all(c <= threshold_white for c in whiteness_check):
        # exclude tile
        return False

    # keep tile
    return True


def crop_rect_from_slide(slide, rect):
    minx, miny, maxx, maxy = rect.bounds
    # Note that the y-axis is flipped in the slide: the top of the shapely polygon is y = ymax,
    # but in the slide it is y = 0. Hence: miny instead of maxy.
    top_left_coords = (int(minx), int(miny))
    return slide.read_region(top_left_coords, 0, (int(maxx - minx), int(maxy - miny)))


class BagOfTiles(Dataset):
    def __init__(self, wsi, tiles, resize_to=224):
        self.wsi = wsi
        self.tiles = tiles

        self.roi_transforms = transforms.Compose(
            [
                # As we can't be sure that the input tile dimensions are all consistent, we resize
                # them to a commonly used size before feeding them to the model.
                # Note: assumes a square image.
                transforms.Resize(resize_to),
                # Turn the PIL image into a (C x H x W) float tensor in the range [0.0, 1.0]
                transforms.ToTensor(),
            ]
        )

    def __len__(self):
        return len(self.tiles)

    def __getitem__(self, idx):
        tile = self.tiles[idx]
        img = crop_rect_from_slide(self.wsi, tile)

        # RGB filtering - calling here speeds up computation since it requires crop_rect_from_slide function.
        is_tile_kept = tile_is_not_empty(img, threshold_white=20)

        # Ensure the img is RGB, as expected by the pretrained model.
        # See https://pytorch.org/docs/stable/torchvision/models.html
        img = img.convert("RGB")

        # Ensure we have a square tile in our hands.
        # We can't handle non-squares currently, as this would requiring changes to
        # the aspect ratio when resizing.
        width, height = img.size
        assert width == height, "input image is not a square"

        img = self.roi_transforms(img).unsqueeze(0)
        coord = tile.bounds
        return img, coord, is_tile_kept


def collate_features(batch):
    # Item 2 is the boolean value from tile filtering.
    img = torch.cat([item[0] for item in batch if item[2]], dim=0)
    coords = np.vstack([item[1] for item in batch if item[2]])
    return [img, coords]


def write_to_h5(file, asset_dict):
    for key, val in asset_dict.items():
        if key not in file:
            maxshape = (None,) + val.shape[1:]
            dset = file.create_dataset(
                key, shape=val.shape, maxshape=maxshape, dtype=val.dtype
            )
            dset[:] = val
        else:
            dset = file[key]
            dset.resize(len(dset) + val.shape[0], axis=0)
            dset[-val.shape[0] :] = val


def load_encoder(backbone, checkpoint_file, use_imagenet_weights, device):
    import torch.nn as nn
    import torchvision.models as models

    class DecapitatedResnet(nn.Module):
        def __init__(self, base_encoder, pretrained):
            super(DecapitatedResnet, self).__init__()
            self.encoder = base_encoder(pretrained=pretrained)

        def forward(self, x):
            # Same forward pass function as used in the torchvision 'stock' ResNet code
            # but with the final FC layer removed.
            x = self.encoder.conv1(x)
            x = self.encoder.bn1(x)
            x = self.encoder.relu(x)
            x = self.encoder.maxpool(x)

            x = self.encoder.layer1(x)
            x = self.encoder.layer2(x)
            x = self.encoder.layer3(x)
            x = self.encoder.layer4(x)

            x = self.encoder.avgpool(x)
            x = torch.flatten(x, 1)

            return x

    model = DecapitatedResnet(models.__dict__[backbone], use_imagenet_weights)

    if use_imagenet_weights:
        if checkpoint_file is not None:
            raise Exception(
                "Either provide a weights checkpoint or the --imagenet flag, not both."
            )
        print(f"Created encoder with Imagenet weights")
    else:
        checkpoint = torch.load(checkpoint_file, map_location="cpu")
        state_dict = checkpoint["state_dict"]
        for k in list(state_dict.keys()):
            # retain only encoder_q up to before the embedding layer
            if k.startswith("module.encoder_q") and not k.startswith(
                "module.encoder_q.fc"
            ):
                # remove prefix from key names
                state_dict[k[len("module.encoder_q.") :]] = state_dict[k]
            # delete renamed or unused k
            del state_dict[k]

        # Verify that the checkpoint did not contain data for the final FC layer
        msg = model.encoder.load_state_dict(state_dict, strict=False)
        assert set(msg.missing_keys) == {"fc.weight", "fc.bias"}
        print(f"Loaded checkpoint {checkpoint_file}")

    model = model.to(device)
    if torch.cuda.device_count() > 1:
        model = torch.nn.DataParallel(model)
    model.eval()

    return model


def extract_features(model, device, wsi, filtered_tiles, workers, out_size, batch_size):
    # Use multiple workers if running on the GPU, otherwise we'll need all workers for
    # evaluating the model.
    kwargs = (
        {"num_workers": workers, "pin_memory": True} if device.type == "cuda" else {}
    )
    loader = DataLoader(
        dataset=BagOfTiles(wsi, filtered_tiles, resize_to=out_size),
        batch_size=batch_size,
        collate_fn=collate_features,
        **kwargs,
    )
    with torch.no_grad():
        for batch, coords in loader:
            batch = batch.to(device, non_blocking=True)
            features = model(batch).cpu().numpy()
            yield features, coords


if __name__ == "__main__":
    import argparse
    import os

    parser = argparse.ArgumentParser(description="Preprocessing script")
    parser.add_argument(
        "--input_slide",
        type=str,
        help="Path to input WSI file",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        help="Directory to save output data",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        help="Feature extractor weights checkpoint",
    )
    parser.add_argument(
        "--backbone",
        type=str,
        help="Backbone of the feature extractor. Should match the shape of the weights file, if provided.",
    )
    parser.add_argument(
        "--imagenet",
        action="store_true",
        help="Use imagenet pretrained weights instead of a custom feature extractor weights checkpoint.",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=512,
    )
    parser.add_argument(
        "--tile_size",
        help="Desired tile size in microns (should be the same value as used in feature extraction model).",
        type=int,
        required=True,
    )
    parser.add_argument(
        "--out_size",
        help="Resize the square tile to this output size (in pixels).",
        type=int,
        default=224,
    )
    parser.add_argument(
        "--workers",
        help="The number of workers to use for the data loader. Only relevant when using a GPU.",
        type=int,
        default=4,
    )
    args = parser.parse_args()

    # Derive the slide ID from its name
    slide_id, _ = os.path.splitext(os.path.basename(args.input_slide))
    wip_file_path = os.path.join(args.output_dir, slide_id + "_wip.h5")
    output_file_path = os.path.join(args.output_dir, slide_id + "_features.h5")

    os.makedirs(args.output_dir, exist_ok=True)

    # Check if the _features output file already exist. If so, we terminate to avoid
    # overwriting it by accident. This also simplifies resuming bulk batch jobs.
    if os.path.exists(output_file_path):
        raise Exception(f"{output_file_path} already exists")

    # Open the slide for reading
    wsi = openslide.open_slide(args.input_slide)

    # Decide on which slide level we want to base the segmentation
    seg_level = wsi.get_best_level_for_downsample(64)

    # Run the segmentation and  tiling procedure
    start_time = time.time()
    tissue_mask_scaled = create_tissue_mask(wsi, seg_level)
    filtered_tiles = create_tissue_tiles(wsi, tissue_mask_scaled, args.tile_size)

    # Build a figure for quality control purposes, to check if the tiles are where we expect them.
    qc_img = make_tile_QC_fig(filtered_tiles, wsi, seg_level, 2)
    qc_img_target_width = 1920
    qc_img = qc_img.resize(
        (qc_img_target_width, int(qc_img.height / (qc_img.width / qc_img_target_width)))
    )
    print(
        f"Finished creating {len(filtered_tiles)} tissue tiles in {time.time() - start_time}s"
    )

    # Extract the rectangles, and compute the feature vectors
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

    model = load_encoder(
        backbone=args.backbone,
        checkpoint_file=args.checkpoint,
        use_imagenet_weights=args.imagenet,
        device=device,
    )

    generator = extract_features(
        model,
        device,
        wsi,
        filtered_tiles,
        args.workers,
        args.out_size,
        args.batch_size,
    )
    start_time = time.time()
    count_features = 0
    with h5py.File(wip_file_path, "w") as file:
        for i, (features, coords) in enumerate(generator):
            count_features += features.shape[0]
            write_to_h5(file, {"features": features, "coords": coords})
            print(
                f"Processed batch {i}. Extracted features from {count_features}/{len(filtered_tiles)} tiles in {(time.time() - start_time):.2f}s."
            )

    # Rename the file containing the patches to ensure we can easily
    # distinguish incomplete bags of patches (due to e.g. errors) from complete ones in case a job fails.
    os.rename(wip_file_path, output_file_path)

    # Save QC figure while keeping track of number of features/tiles used since RBG filtering is within DataLoader.
    qc_img_file_path = os.path.join(
        args.output_dir, f"{slide_id}_{count_features}_features_QC.png"
    )
    qc_img.save(qc_img_file_path)
    print(
        f"Finished extracting {count_features} features in {(time.time() - start_time):.2f}s"
    )
```
sample_tiles.py
```python
import argparse
import os
import random

import openslide

from preprocess import (
    create_tissue_mask,
    create_tissue_tiles,
    crop_rect_from_slide,
    make_tile_QC_fig,
    tile_is_not_empty,
)

parser = argparse.ArgumentParser(
    description="Script to sample tiles from a WSI and save them as individual image files"
)
parser.add_argument("--input_slide", type=str, help="Path to input WSI file")
parser.add_argument(
    "--output_dir", type=str, help="Directory to save output tile files"
)
parser.add_argument(
    "--tile_size", help="desired tile size in microns", type=int, required=True
)
parser.add_argument("--n", help="numer of tiles to sample", type=int, default=2048)
parser.add_argument("--seed", help="seed for RNG", type=int, default=42)
parser.add_argument(
    "--out_size",
    help="resize the square tile to this output size (in pixels)",
    type=int,
    default=224,
)
args = parser.parse_args()

random.seed(args.seed)

slide_id, _ = os.path.splitext(os.path.basename(args.input_slide))
wsi = openslide.open_slide(args.input_slide)

QC_DIR = os.path.join(args.output_dir, "QC")
TILE_DIR = os.path.join(args.output_dir, "train")
slide_dir = os.path.join(TILE_DIR, slide_id)

os.makedirs(QC_DIR, exist_ok=True)
os.makedirs(TILE_DIR, exist_ok=True)
os.makedirs(slide_dir, exist_ok=True)

# Decide on which slide level we want to base the segmentation
seg_level = wsi.get_best_level_for_downsample(64)

tissue_mask_scaled = create_tissue_mask(wsi, seg_level)
filtered_tiles = create_tissue_tiles(wsi, tissue_mask_scaled, args.tile_size)

# RGB filtering to detect low-entropy tiles. This is slow, because it requires all tiles to be loaded.
filtered_tiles = [
    rect
    for rect in filtered_tiles
    if tile_is_not_empty(crop_rect_from_slide(wsi, rect), threshold_white=20)
]

# When the number of tiles to sample is greater than or equal to the total number of tiles in the slide, we take all of them.
sampled_tiles = random.sample(filtered_tiles, min(args.n, len(filtered_tiles)))

print(
    f"Sampled {len(sampled_tiles)} tiles out of {len(filtered_tiles)} non-empty tiles."
)

# Build a figure for quality control purposes, to check if the tiles are where we expect them.
qc_img = make_tile_QC_fig(filtered_tiles, wsi, seg_level, 2, extra_tiles=sampled_tiles)
qc_img_target_width = 1920
qc_img = qc_img.resize(
    (qc_img_target_width, int(qc_img.height / (qc_img.width / qc_img_target_width)))
)
qc_img_file_path = os.path.join(
    QC_DIR, f"{slide_id}_sampled{len(sampled_tiles)}_{len(filtered_tiles)}tiles_QC.png"
)
qc_img.save(qc_img_file_path)

for i, tile in enumerate(sampled_tiles):
    img = crop_rect_from_slide(wsi, tile)

    # Ensure we have a square tile in our hands.
    # We can't handle non-squares currently, as this would requiring changes to
    # the aspect ratio when resizing.
    width, height = img.size
    assert width == height, "input image is not a square"

    img = img.resize((args.out_size, args.out_size))

    # Convert from RGBA to RGB (don't care about the alpha channel anyway)
    img = img.convert("RGB")

    out_file = os.path.join(slide_dir, f"{slide_id}_tile_{i}.png")
    img.save(out_file, format="png", quality=100, subsampling=0)
```
运行方式:
```sh
python sample_tiles.py \
--input_slide /path/to/your/WSI.svs \
--output_dir moco_tiles \
--tile_size 360 \
--n 2048 \
--out_size 224
```
在切片大小的选择上，tile_size
Previous studies adopted tile size of 300 × 300 pixels at 40x, 512 × 512 pixels at 20x , 600 × 600 pixels (an equivalent of 9 × 10−3 mm2) 16)] for [CNN training](https://www.sciencedirect.com/topics/computer-science/neural-network-training "Learn more about CNN training from ScienceDirect's AI-generated Topic Pages") on [histopathological images](https://www.sciencedirect.com/topics/computer-science/histopathological-image "Learn more about histopathological images from ScienceDirect's AI-generated Topic Pages").
Combining the findings in quantitative and qualitative assessments, optimal tile size was between 500 and 1000 pixel (125–250 μm).
### 染色归一化
注意，需要python降级3.8才能使用
[（染色归一化）病理图像（HE或者WSI）图像标准化方法小介绍_wsi图像-CSDN博客](https://blog.csdn.net/qq_34616741/article/details/100541893)
```python
import spams
import numpy as np
import cv2
import time


class vahadane(object):

    def __init__(
        self,
        STAIN_NUM=2,
        THRESH=0.9,
        LAMBDA1=0.01,
        LAMBDA2=0.01,
        ITER=100,
        fast_mode=0,
        getH_mode=0,
    ):
        self.STAIN_NUM = STAIN_NUM
        self.THRESH = THRESH
        self.LAMBDA1 = LAMBDA1
        self.LAMBDA2 = LAMBDA2
        self.ITER = ITER
        self.fast_mode = fast_mode  # 0: normal; 1: fast
        self.getH_mode = getH_mode  # 0: spams.lasso; 1: pinv;

    def show_config(self):
        print("STAIN_NUM =", self.STAIN_NUM)
        print("THRESH =", self.THRESH)
        print("LAMBDA1 =", self.LAMBDA1)
        print("LAMBDA2 =", self.LAMBDA2)
        print("ITER =", self.ITER)
        print("fast_mode =", self.fast_mode)
        print("getH_mode =", self.getH_mode)

    def getV(self, img):
        I0 = img.reshape((-1, 3)).T
        I0[I0 == 0] = 1
        V0 = np.log(255 / I0)
        img_LAB = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
        mask = (img_LAB[:, :, 0] / 255 < self.THRESH).astype(bool)
        I = img[mask].reshape((-1, 3)).T
        I[I == 0] = 1
        V = np.log(255 / I)
        return V0, V

    def getW(self, V):
        W = spams.trainDL(
            np.asfortranarray(V),
            K=self.STAIN_NUM,
            lambda1=self.LAMBDA1,
            iter=self.ITER,
            mode=2,
            modeD=0,
            posAlpha=True,
            posD=True,
            verbose=False,
        )
        W = W / np.linalg.norm(W, axis=0)[None, :]
        if W[0, 0] < W[0, 1]:
            W = W[:, [1, 0]]
        return W

    def getH(self, V, W):
        if self.getH_mode == 0:
            H = spams.lasso(
                np.asfortranarray(V),
                np.asfortranarray(W),
                mode=2,
                lambda1=self.LAMBDA2,
                pos=True,
                verbose=False,
            ).toarray()
        elif self.getH_mode == 1:
            H = np.linalg.pinv(W).dot(V)
            H[H < 0] = 0
        else:
            H = 0
        return H

    def stain_separate(self, img):
        start = time.time()
        if self.fast_mode == 0:
            V0, V = self.getV(img)
            W = self.getW(V)
            H = self.getH(V0, W)
        elif self.fast_mode == 1:
            m = img.shape[0]
            n = img.shape[1]
            grid_size_m = int(m / 5)
            lenm = int(m / 20)
            grid_size_n = int(n / 5)
            lenn = int(n / 20)
            W = np.zeros((81, 3, self.STAIN_NUM)).astype(np.float64)
            for i in range(0, 4):
                for j in range(0, 4):
                    px = (i + 1) * grid_size_m
                    py = (j + 1) * grid_size_n
                    patch = img[px - lenm : px + lenm, py - lenn : py + lenn, :]
                    V0, V = self.getV(patch)
                    W[i * 9 + j] = self.getW(V)
            W = np.mean(W, axis=0)
            V0, V = self.getV(img)
            H = self.getH(V0, W)
        print("stain separation time:", time.time() - start, "s")
        return W, H

    def SPCN(self, img, Ws, Hs, Wt, Ht):
        Hs_RM = np.percentile(Hs, 99)
        Ht_RM = np.percentile(Ht, 99)
        Hs_norm = Hs * Ht_RM / Hs_RM
        Vs_norm = np.dot(Wt, Hs_norm)
        Is_norm = 255 * np.exp(-1 * Vs_norm)
        I = Is_norm.T.reshape(img.shape).astype(np.uint8)
        return I


# 初始化 vahadane 对象
vahadane_obj = vahadane()

# 加载源图片和目标图片
source_image = cv2.imread(
    "../../data/PKG_UPENN_GBM_v2/NDPI_images_processed/7316UP-109/train/7316UP-109/7316UP-109_tile_10.png"
)
target_image = cv2.imread(
    "../../data/PKG_UPENN_GBM_v2/NDPI_images_processed/7316UP-1110/train/7316UP-1110/7316UP-1110_tile_103.png"
)

# 确保图片是RGB格式
source_image = cv2.cvtColor(source_image, cv2.COLOR_BGR2RGB)
target_image = cv2.cvtColor(target_image, cv2.COLOR_BGR2RGB)

# 分离源图片和目标图片的染色基矩阵和染色浓度矩阵
Ws, Hs = vahadane_obj.stain_separate(source_image)
Wt, Ht = vahadane_obj.stain_separate(target_image)

# 将源图片的染色浓度矩阵和目标图片的染色基矩阵进行匹配
normalized_image = vahadane_obj.SPCN(source_image, Ws, Hs, Wt, Ht)

# 保存匹配后的图片
normalized_image_bgr = cv2.cvtColor(normalized_image, cv2.COLOR_RGB2BGR)
cv2.imwrite("normalized_image.jpg", normalized_image_bgr)
```
